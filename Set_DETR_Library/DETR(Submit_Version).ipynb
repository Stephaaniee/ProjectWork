{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e77ec2a-828a-4bab-8590-f3e0eedc83fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8e4c08a81ae39056\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8e4c08a81ae39056\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training examples: 448\n",
      "Number of testing examples: 63\n",
      "Number of validation examples: 127\n",
      "image = #325\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4392), started 0:00:04 ago. (Use '!kill 4392' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-42a3215a413d37dd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-42a3215a413d37dd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([9, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No supported gpu backend found!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 220\u001b[0m\n\u001b[0;32m    218\u001b[0m MAXEPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m#trainer = Trainer(max_epochs=MAXEPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAXEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_clip_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m#trainer = Trainer(max_steps=300, gradient_clip_val=0.1)\u001b[39;00m\n\u001b[0;32m    223\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\utilities\\argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\trainer\\trainer.py:400\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:147\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_auto_accelerator()\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_choose_gpu_accelerator_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_device_config_and_set_final_flags(devices\u001b[38;5;241m=\u001b[39mdevices, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_parallel_devices_and_init_accelerator()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:367\u001b[0m, in \u001b[0;36m_AcceleratorConnector._choose_gpu_accelerator_backend\u001b[1;34m()\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CUDAAccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported gpu backend found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: No supported gpu backend found!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import supervision #used to annotate frame and manage vision\n",
    "import transformers #load detr model\n",
    "import pytorch_lightning as pl #manage data training \n",
    "import os #access GPU\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection  # the first is the utilities used in preprocessing the image and post processing the detection, and the second command is the actual model\n",
    "import matplotlib\n",
    "import cv2\n",
    "import random\n",
    "import timm #load model backbone\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n",
    "%matplotlib inline\n",
    "from pytorch_lightning import Trainer #Trigger the training\n",
    "#References\n",
    "#https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb#scrollTo=pzOJhEPa39dZ\n",
    "\n",
    "\n",
    "#step 1: Load dataset\n",
    "model_dataset = r\"C:\\Users\\49152\\Downloads\\Aquarium Combined.v2-raw-1024.coco\"\n",
    "train_dataset = os.path.join(model_dataset, 'train')\n",
    "test_dataset = os.path.join(model_dataset, 'test')\n",
    "valid_dataset = os.path.join(model_dataset, 'valid')\n",
    "annotation_file_name = \"_annotations.coco.json\"\n",
    "\n",
    "#step 2: using the coco class to load the dataset \n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self,\n",
    "                 image_dir_path:str,\n",
    "                 image_processor,\n",
    "                 train: bool=True):\n",
    "        annotation_file_path = os.path.join(image_dir_path, annotation_file_name)\n",
    "        super(CocoDetection, self).__init__(image_dir_path, annotation_file_path)\n",
    "        #preprocess images before parsing through neural network\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        # feel free to add data augmentation here before passing them to the next step\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.image_processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target\n",
    "# step 3: define image_processor(for postprocessing)\n",
    "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "\n",
    "#step 4: create CocoDetection class for our data\n",
    "train_cocoData = CocoDetection(image_dir_path = train_dataset, image_processor=image_processor, train=True)\n",
    "test_cocoData = CocoDetection(image_dir_path = test_dataset, image_processor=image_processor, train=False)  \n",
    "valid_cocoData = CocoDetection(image_dir_path = valid_dataset, image_processor=image_processor, train=False)\n",
    "\n",
    "# To confirm data available for training , testing and validation\n",
    "print(\"Number of training examples:\", len(train_cocoData))\n",
    "print(\"Number of testing examples:\", len(test_cocoData))\n",
    "print(\"Number of validation examples:\", len(valid_cocoData))\n",
    "\n",
    "#Step 5: To test that the data loaded properly, we load a random dataset and visualize from our model\n",
    "image_ids = train_cocoData.coco.getImgIds()\n",
    "#get ids of all images in train dataset and randomly select an image\n",
    "image_id = random.choice(image_ids)\n",
    "print('image = #{}'.format(image_id))\n",
    "\n",
    "#step 6: load image and annotation\n",
    "image= train_cocoData.coco.loadImgs(image_id)[0]\n",
    "annotation = train_cocoData.coco.imgToAnns[image_id]\n",
    "image_path = os.path.join(train_cocoData.root, image['file_name'])\n",
    "image= cv2.imread(image_path)\n",
    "\n",
    "# step 7: annotation : use supervision to convert coco annotations into detection objects used in annotating bounding boxes on the source image\n",
    "detections = supervision.Detections.from_coco_annotations(coco_annotation=annotation)\n",
    "\n",
    "#training\n",
    "categories = train_cocoData.coco.cats\n",
    "id2label = {k: v['name'] for k,v in categories.items()}\n",
    "\n",
    "labels = [\n",
    "    f\"{id2label[class_id]}\"\n",
    "    for _, _, class_id, _\n",
    "    in detections\n",
    "]\n",
    "box_annotator = supervision.BoxAnnotator()\n",
    "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
    "\n",
    "#step 8 visualizing our random image\n",
    "# #get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "# supervision.show_frame_in_notebook(image, (8,8))\n",
    "\n",
    "#responsible for easing training and conducting inferencd in the neural network\n",
    "# step 9: images are padded when training images of different sizes in a batch. \n",
    "#The images are padded to the biggest resolution in the batch, creating a corresponinding binary_pixel which is used to indicate which pixel are/are not real\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return{\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'], #stores only 1's and 0's and determines if the pixel was originally in the image or not\n",
    "        'labels': labels\n",
    "    }\n",
    "#selecting a batch size has a direct impact on the speed of the training and memory allocation\n",
    "DataLoader_Train= DataLoader(dataset= train_cocoData, collate_fn = collate_fn, batch_size = 4, shuffle=True)\n",
    "DataLoader_Test= DataLoader(dataset = test_cocoData, collate_fn = collate_fn)\n",
    "DataLoader_Valid= DataLoader(dataset = valid_cocoData, collate_fn = collate_fn)\n",
    "\n",
    "#step 10: Model Training: For every data model trainig, we do a loss calculation \n",
    "#to infer predicted value from the actual value. An optimizer is used to give optimized solutions on how to better the predicted value to move closer to the actual/desired output\n",
    "# Then we perform backward propagation i.e tweaking the weight to values to move the predicted value closer to the actual output\n",
    "# We use pytorch-lightening library to achieve this\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         #initialize the model with image_processor i.e. resnet\n",
    "         super().__init__()\n",
    "         \n",
    "         # replace COCO classification head with custom head\n",
    "         # we specify the \"no_timm\" variant here to not rely on the timm library\n",
    "         # for the convolutional backbone\n",
    "         \n",
    "         self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n",
    "                                                             revision=\"no_timm\",\n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         \n",
    "        # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "        # DeTR uses the backbone learning rate() to apply first to the whole network and the only to the backbone\n",
    "        # This has direct influence on the optimizer result\n",
    "         # the lr decides the size of the step taken during training iteration to reach the deired output\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "\n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics , and average across each epoc for each training step\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"valid_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    #pass train and validation data loaders\n",
    "     def DataLoader_Train(self):\n",
    "        return DataLoader_Train\n",
    "\n",
    "     def DataLoader_Valid(self):\n",
    "        return DataLoader_Valid\n",
    "\n",
    "# Start training.\n",
    "    #Tensor boards are used in tracking the key matrix during training\n",
    "    #create an instance of pytorch module \n",
    "    # Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n",
    "\n",
    "\n",
    "\n",
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    " #pass one batch through the module\n",
    "batch = next(iter(DataLoader_Train))\n",
    "\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n",
    "\n",
    "#Step 11: Trigger the training\n",
    "    #set EPOCHS\n",
    "MAXEPOCHS = 200\n",
    "#trainer = Trainer(max_epochs=MAXEPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "trainer = Trainer(devices=1, accumulate_grad_batches= 8, max_epochs= MAXEPOCHS, accelerator=\"gpu\", gradient_clip_algorithm=0.1, log_every_n_steps=5)\n",
    "\n",
    "#trainer = Trainer(max_steps=300, gradient_clip_val=0.1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab3c5d0-9742-4794-a197-9b1380a34e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d5f9f-2fcb-4dd9-b795-a60ff46bfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84abb7-9fc7-4add-9efc-74dca7ec6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9f181-adf5-4656-b131-702abde6d528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
